


class SGD:
    def __init__(self, lr=0.01):
        self.lr = lr # learning rate

    def update(self, params, grads):
        for key in params.keys():
            params[key] -= self.lr * grads[key]





# 등방성
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# range of x, y
x = np.linspace(-5, 5, 20)
y = np.linspace(-5, 5, 20)
X, Y = np.meshgrid(x, y)
Z = X**2 + Y**2

# calc slope
dx,dy = np.gradient(Z)

# Graph
fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection='3d')
ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.7, edgecolor='none')

ax.set_xlabel('X axis')
ax.set_ylabel('Y axis')
ax.set_zlabel('Z axis')
ax.set_title('3D Plot of z = x^2 + y^2 with Gradient Vectors')

plt.show()



import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# X, Y 축의 범위와 좌표 생성
x = np.linspace(-5, 5, 20)
y = np.linspace(-5, 5, 20)
X, Y = np.meshgrid(x, y)
Z = X**2 + Y**2

# 기울기 계산 (수식을 직접 사용)
dx = 2 * X
dy = 2 * Y

# 3차원 그래프 그리기
fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection='3d')
ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.7, edgecolor='none')

# 2차원 화살표로 기울기 시각화 (z=0 평면에서 그리기)
ax.quiver(X, Y, 0, dx, dy, 0, color='red', length=0.1, normalize=False, arrow_length_ratio=0.3)

# 그래프 설정
ax.set_xlabel('X axis')
ax.set_ylabel('Y axis')
ax.set_zlabel('Z axis')
ax.set_title('3D Plot of z = x^2 + y^2 with 2D Gradient Vectors')

plt.show()



# 비등방성
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# X, Y 축의 범위와 좌표 생성
x = np.linspace(-5, 5, 20)
y = np.linspace(-5, 5, 20)
X, Y = np.meshgrid(x, y)
Z = (1/20) * X**2 + Y**2

# 기울기 계산 (수식을 직접 사용)
dx = (1/10) * X  # x 방향 편미분
dy = 2 * Y       # y 방향 편미분

# 3차원 그래프 그리기
fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection='3d')
ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.7, edgecolor='none')

# 2차원 화살표로 기울기 시각화 (z=0 평면에서 그리기)
ax.quiver(X, Y, 0, dx, dy, 0, color='red', length=0.2, normalize=False, arrow_length_ratio=0.3)

# 그래프 설정
ax.set_xlabel('X axis')
ax.set_ylabel('Y axis')
ax.set_zlabel('Z axis')
ax.set_title(r'3D Plot of $z = \frac{1}{20}x^2 + y^2$ with 2D Gradient Vectors')

plt.show()



def _numerical_gradient_no_batch(f, x):
    h = 1e-4 # 0.0001
    grad = np.zeros_like(x) # x와 형상이 같은 배열을 생성
    
    for idx in range(x.size):
        tmp_val = x[idx]
        
        # f(x+h) 계산
        x[idx] = float(tmp_val) + h
        fxh1 = f(x)
        
        # f(x-h) 계산
        x[idx] = tmp_val - h 
        fxh2 = f(x) 
        
        grad[idx] = (fxh1 - fxh2) / (2*h)
        x[idx] = tmp_val # 값 복원
        
    return grad
    
# f(x, y) = (1/20) * x**2 + y**2 의 기울기
from mpl_toolkits.mplot3d import Axes3D

def numerical_gradient(f, X):
    if X.ndim == 1:
        return _numerical_gradient_no_batch(f, X)
    else:
        grad = np.zeros_like(X)
        
        for idx, x in enumerate(X):
            grad[idx] = _numerical_gradient_no_batch(f, x)
        
        return grad

def function_2(x):
    if x.ndim == 1:
        return np.sum(x**2)
    else:
        return np.sum(x**2, axis=1)
     
x0 = np.arange(-10, 10, 1)
x1 = np.arange(-10, 10, 1)
X, Y = np.meshgrid(x0, x1)
    
X = X.flatten()
Y = Y.flatten()

grad = numerical_gradient(function_2, np.array([(1/(20**0.5))*X, Y]) )
    
plt.figure()
plt.quiver(X, Y, -grad[0], -grad[1],  angles="xy",color="#666666")#,headwidth=10,scale=40,color="#444444")
plt.xlim([-10, 10])
plt.ylim([-5, 5])
plt.xlabel('x0')
plt.ylabel('x1')
plt.grid()
plt.legend()
plt.draw()
plt.show()





import numpy as np
import matplotlib.pyplot as plt

# 등방성 함수와 그 기울기 정의
def isotropic_function(x, y):
    return x**2 + y**2

def isotropic_gradient(x, y):
    dx = 2 * x  # x 방향 편미분
    dy = 2 * y  # y 방향 편미분
    return dx, dy

# 비등방성 함수와 그 기울기 정의
def anisotropic_function(x, y):
    return (1/20) * x**2 + y**2

def anisotropic_gradient(x, y):
    dx = (1/10) * x  # x 방향 편미분
    dy = 2 * y       # y 방향 편미분
    return dx, dy

# SGD 경로 생성 함수
def sgd_path(function, gradient, start_x, start_y, learning_rate, steps, noise_level=0.05):
    x, y = start_x, start_y
    path_x, path_y, path_z = [x], [y], [function(x, y)]

    for _ in range(steps):
        dx, dy = gradient(x, y)
        # 작은 무작위 노이즈를 추가하여 지그재그 경로를 생성
        x -= learning_rate * (dx + np.random.normal(0, noise_level))
        y -= learning_rate * (dy + np.random.normal(0, noise_level))
        
        path_x.append(x)
        path_y.append(y)
        path_z.append(function(x, y))
    
    return path_x, path_y, path_z

# 시각화 설정
np.random.seed(0)
start_x, start_y = -10.0, 10.0  # 시작점
learning_rate = 0.3
steps = 24
noise_level = 0.02  # 노이즈 레벨을 낮춤

# 등방성 함수에서 SGD 경로 생성
iso_path_x, iso_path_y, iso_path_z = sgd_path(isotropic_function, isotropic_gradient, start_x, start_y, learning_rate, steps, noise_level)

# 비등방성 함수에서 SGD 경로 생성
anisotropic_path_x, anisotropic_path_y, anisotropic_path_z = sgd_path(anisotropic_function, anisotropic_gradient, start_x, start_y, learning_rate, steps, noise_level)

# 큰 범위 설정
x_vals = np.linspace(-10, 10, 100)
y_vals = np.linspace(-10, 10, 100)
X, Y = np.meshgrid(x_vals, y_vals)

# 등방성 함수의 등고선
Z_iso = isotropic_function(X, Y)
plt.figure(figsize=(12, 6))
plt.contour(X, Y, Z_iso, levels=50, cmap='Blues', alpha=0.6)
plt.plot(iso_path_x, iso_path_y, 'bo-', markersize=4, label='SGD Path (Isotropic)')
plt.quiver(iso_path_x[:-1], iso_path_y[:-1], np.diff(iso_path_x), np.diff(iso_path_y), angles='xy', scale_units='xy', scale=1, color='blue', width=0.003)

# 비등방성 함수의 등고선
Z_aniso = anisotropic_function(X, Y)
plt.contour(X, Y, Z_aniso, levels=50, cmap='Oranges', alpha=0.6)
plt.plot(anisotropic_path_x, anisotropic_path_y, 'ro-', markersize=4, label='SGD Path (Anisotropic)')
plt.quiver(anisotropic_path_x[:-1], anisotropic_path_y[:-1], np.diff(anisotropic_path_x), np.diff(anisotropic_path_y), angles='xy', scale_units='xy', scale=1, color='red', width=0.003)

# 설정
plt.xlabel('x')
plt.ylabel('y')
plt.title('SGD Path Comparison on Isotropic and Anisotropic Functions')
plt.legend()
plt.grid(True)
plt.xlim(-10, 10)
plt.ylim(-10, 10)
plt.show()









class Momentum:
    def __init__(self, lr=0.01, momentum=0.9):
        self.lr = lr # learning rate
        self.momentum = momentum
        self.v = None # velocity

    def update(self, params, grads):
        if self.v is None:
            self.v = {}
            for key, val in params.items():
                self.v[key] = np.zeros_like(val)
        for key in params.keys():
            self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]
            params[key] += self.v[key]









class AdaGrad:
    def __init__(self, lr=0.01):
        self.lr = lr
        self.h = None
        
    def update(self, params, grads):
        if self.h is None:
            self.h = {}
            for key, val in params.items():
                self.h[key] = np.zeros_like(val)
            
        for key in params.keys():
            self.h[key] += grads[key] * grads[key]
            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)


def function(x, y):
    return 0.05 * x**2 + y**2

def gradient(x, y):
    return 0.1 * x, 2 * y  # 기울기
    
def plot_optimization_path(optimizer, title):
    params = {'x': -7.0, 'y': 7.0}  # 초기 값
    path_x, path_y = [params['x']], [params['y']]

    for _ in range(30):
        grads = {}
        grads['x'], grads['y'] = gradient(params['x'], params['y'])
        optimizer.update(params, grads)
        
        path_x.append(params['x'])
        path_y.append(params['y'])

    # 등고선 그리기
    x_vals = np.linspace(-10, 10, 100)
    y_vals = np.linspace(-10, 10, 100)
    X, Y = np.meshgrid(x_vals, y_vals)
    Z = function(X, Y)

    plt.contour(X, Y, Z, levels=50, cmap='viridis')
    plt.plot(path_x, path_y, 'o-', color='red', markersize=4, label=title)
    plt.quiver(path_x[:-1], path_y[:-1], np.diff(path_x), np.diff(path_y), angles='xy', scale_units='xy', scale=1, color='black', width=0.003)
    plt.xlabel('x')
    plt.ylabel('y')
    plt.title(f'SGD Path using {title}')
    plt.legend()
    plt.grid(True)
    plt.xlim(-10, 10)
    plt.ylim(-10, 10)
    plt.show()

# 모멘텀과 AdaGrad로 최적화 경로 시각화
momentum_optimizer = Momentum(lr=0.1, momentum=0.9)
adagrad_optimizer = AdaGrad(lr=1.5)

plot_optimization_path(momentum_optimizer, "Momentum")
plot_optimization_path(adagrad_optimizer, "AdaGrad")






class Adam:
    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):
        self.lr = lr
        self.beta1 = beta1
        self.beta2 = beta2
        self.iter = 0
        self.m = None
        self.v = None
        
    def update(self, params, grads):
        if self.m is None:
            self.m, self.v = {}, {}
            for key, val in params.items():
                self.m[key] = np.zeros_like(val)
                self.v[key] = np.zeros_like(val)
        
        self.iter += 1
        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         
        
        for key in params.keys():
            #self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]
            #self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)
            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])
            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])
            
            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)
            
            #unbias_m += (1 - self.beta1) * (grads[key] - self.m[key]) # correct bias
            #unbisa_b += (1 - self.beta2) * (grads[key]*grads[key] - self.v[key]) # correct bias
            #params[key] += self.lr * unbias_m / (np.sqrt(unbisa_b) + 1e-7)













# coding: utf-8
import numpy as np
import matplotlib.pyplot as plt


def sigmoid(x):
    return 1 / (1 + np.exp(-x))


def ReLU(x):
    return np.maximum(0, x)


def tanh(x):
    return np.tanh(x)
    
input_data = np.random.randn(1000, 100)  # 1000개의 데이터
node_num = 100  # 각 은닉층의 노드(뉴런) 수
hidden_layer_size = 5  # 은닉층이 5개
activations = {}  # 이곳에 활성화 결과를 저장

x = input_data

for i in range(hidden_layer_size):
    if i != 0:
        x = activations[i-1]

    # 초깃값을 다양하게 바꿔가며 실험해보자！
    w0 = np.random.randn(node_num, node_num) * 1
    w1 = np.random.randn(node_num, node_num) * 0.01
    # w = np.random.randn(node_num, node_num) * np.sqrt(1.0 / node_num)
    # w = np.random.randn(node_num, node_num) * np.sqrt(2.0 / node_num)


    a0 = np.dot(x, w0)
    a1 = np.dot(x, w1)


    # 활성화 함수도 바꿔가며 실험해보자！
    z = sigmoid(a)
    # z = ReLU(a)
    # z = tanh(a)

    activations[i] = z

# 히스토그램 그리기
for i, a0 in activations.items():
    plt.subplot(1, len(activations), i+1)
    plt.title(str(i+1) + "-layer")
    if i != 0: plt.yticks([], [])
    # plt.xlim(0.1, 1)
    # plt.ylim(0, 7000)
    plt.hist(a.flatten(), 30, range=(0,1))

plt.show()





# coding: utf-8
import numpy as np
import matplotlib.pyplot as plt

# 활성화 함수 정의
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def ReLU(x):
    return np.maximum(0, x)

def tanh(x):
    return np.tanh(x)

# 설정
input_data = np.random.randn(1000, 100)  # 1000개의 데이터
node_num = 100  # 각 은닉층의 노드(뉴런) 수
hidden_layer_size = 5  # 은닉층이 5개

# 실험할 초기화 방법과 활성화 함수 정의
initialization_methods = {
    "std=1": lambda: np.random.randn(node_num, node_num) * 1,
    "std=0.01": lambda: np.random.randn(node_num, node_num) * 0.01,
    "Xavier": lambda: np.random.randn(node_num, node_num) * np.sqrt(1.0 / node_num),
    "He": lambda: np.random.randn(node_num, node_num) * np.sqrt(2.0 / node_num)
}

activation_functions = {
    "sigmoid": sigmoid,
    "ReLU": ReLU,
    "tanh": tanh
}

# 각 초기화 및 활성화 함수 조합에 대해 레이어별 활성화 분포 시각화
for init_name, init_func in initialization_methods.items():
    for act_name, act_func in activation_functions.items():
        x = input_data
        activations = {}

        for i in range(hidden_layer_size):
            if i != 0:
                x = activations[i - 1]

            # 초기화와 활성화 함수 적용
            w = init_func()
            a = np.dot(x, w)
            z = act_func(a)

            activations[i] = z

        # 그래프 생성
        fig, axes = plt.subplots(1, hidden_layer_size, figsize=(15, 3))
        fig.suptitle(f"Initialization: {init_name}, Activation: {act_name}")

        # 각 레이어의 히스토그램 그리기
        for i in range(hidden_layer_size):
            ax = axes[i]
            ax.hist(activations[i].flatten(), bins=30, range=(0, 1), color='b', alpha=0.7)
            ax.set_title(f"Layer {i+1}")
            ax.set_yticks([])

        plt.tight_layout(rect=[0, 0, 1, 1])  # 제목을 위한 여백 조정
        plt.show()









input_data = np.random.randn(1000, 300)  # 1000개의 데이터
node_num = 300  # 각 은닉층의 노드(뉴런) 수
hidden_layer_size = 4  # 은닉층이 5개

activation_functions = {
    #"sigmoid": sigmoid,
    "ReLU": ReLU,
    #"tanh": tanh
}

for init_name, init_func in initialization_methods.items():
    for act_name, act_func in activation_functions.items():
        x = input_data
        activations = {}

        for i in range(hidden_layer_size):
            if i != 0:
                x = activations[i - 1]

            # 초기화와 활성화 함수 적용
            w = init_func()
            a = np.dot(x, w)
            z = act_func(a)

            activations[i] = z

        # 그래프 생성
        fig, axes = plt.subplots(1, hidden_layer_size, figsize=(9, 3))
        fig.suptitle(f"Initialization: {init_name}, Activation: {act_name}")

        # 각 레이어의 히스토그램 그리기
        for i in range(hidden_layer_size):
            ax = axes[i]
            ax.hist(activations[i].flatten(), bins=20, range=(0, 1), color='b', alpha=0.7)
            ax.set_title(f"Layer {i+1}")
            ax.set_yticks([])

        plt.tight_layout(rect=[0, 0, 1, 1])  # 제목을 위한 여백 조정
        plt.show()

























